{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "import random\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "path_dataset = os.path.join('/Users/student/Desktop/', 'aclImdb')\n",
    "\n",
    "train_txt = []\n",
    "train_labels = []\n",
    "for category in ['pos', 'neg']:\n",
    "    train_path = os.path.join(path_dataset, 'train', category)\n",
    "    train_lst = sorted(os.listdir(train_path))\n",
    "    for name in train_lst:\n",
    "        if name.endswith('.txt'):\n",
    "            with open(os.path.join(train_path, name)) as file:\n",
    "                train_txt.append(file.read())\n",
    "            binary_train = (0 if category == 'neg' else 1)    \n",
    "            train_labels.append(binary_train)\n",
    "\n",
    "\n",
    "sample_texts = train_txt\n",
    "ngram_range=(1, 2)\n",
    "num_ngrams=10\n",
    "\n",
    "## Transforming words into a matrix of vectors for analysis \n",
    "kwargs = {'ngram_range': (1, 1), \n",
    "          'dtype': 'int32',\n",
    "          'strip_accents': 'unicode',\n",
    "          'decode_error': 'replace',\n",
    "          'analyzer': 'word',}\n",
    "          \n",
    "vectorizer = CountVectorizer(**kwargs)\n",
    "vectorized_texts = vectorizer.fit_transform(sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, valid_data,train_lab, valid_lab =  train_test_split(train_txt, train_labels,train_size=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 20000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(train_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data), len(valid_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1799: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<20000x20000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 2776908 stored elements in Compressed Sparse Row format>,\n",
       " <5000x20000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 673875 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2  \n",
    " \n",
    "\n",
    "# Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "kwargs = {\n",
    "    'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "    'dtype': 'int32',\n",
    "    'strip_accents': 'unicode',\n",
    "    'decode_error': 'replace',\n",
    "    'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "    'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "}\n",
    "vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "# Learn vocabulary from training texts and vectorize training texts.\n",
    "x_train = vectorizer.fit_transform(train_data)\n",
    "\n",
    "# Vectorize validation texts.\n",
    "x_val = vectorizer.transform(valid_data)\n",
    "\n",
    "# Select top 'k' of the vectorized features.\n",
    "selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "selector.fit(x_train, train_lab)\n",
    "x_train = selector.transform(x_train).astype('float32')\n",
    "x_val = selector.transform(x_val).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 242)\t0.031029455\n",
      "  (0, 368)\t0.061260045\n",
      "  (0, 678)\t0.015807707\n",
      "  (0, 710)\t0.07234268\n",
      "  (0, 772)\t0.05707789\n",
      "  (0, 1044)\t0.05677071\n",
      "  (0, 1371)\t0.022692041\n",
      "  (0, 1632)\t0.026414163\n",
      "  (0, 1712)\t0.044213902\n",
      "  (0, 1761)\t0.015225832\n",
      "  (0, 1781)\t0.023393383\n",
      "  (0, 1958)\t0.022411503\n",
      "  (0, 2698)\t0.06738119\n",
      "  (0, 2963)\t0.035281267\n",
      "  (0, 3008)\t0.012210258\n",
      "  (0, 3081)\t0.016103787\n",
      "  (0, 3832)\t0.047356665\n",
      "  (0, 4231)\t0.07749398\n",
      "  (0, 4479)\t0.027389573\n",
      "  (0, 4572)\t0.05685181\n",
      "  (0, 4889)\t0.05685181\n",
      "  (0, 4943)\t0.032842755\n",
      "  (0, 4947)\t0.07484585\n",
      "  (0, 5020)\t0.052515224\n",
      "  (0, 5128)\t0.06664439\n",
      "  :\t:\n",
      "  (19999, 18535)\t0.025769433\n",
      "  (19999, 18655)\t0.036990523\n",
      "  (19999, 18808)\t0.017222002\n",
      "  (19999, 18944)\t0.046470046\n",
      "  (19999, 19048)\t0.023737187\n",
      "  (19999, 19099)\t0.029700674\n",
      "  (19999, 19100)\t0.034025345\n",
      "  (19999, 19144)\t0.02381657\n",
      "  (19999, 19216)\t0.0144394925\n",
      "  (19999, 19237)\t0.046156477\n",
      "  (19999, 19250)\t0.019103944\n",
      "  (19999, 19286)\t0.024407027\n",
      "  (19999, 19353)\t0.012301375\n",
      "  (19999, 19371)\t0.04549499\n",
      "  (19999, 19423)\t0.049016304\n",
      "  (19999, 19513)\t0.0124913985\n",
      "  (19999, 19515)\t0.020621989\n",
      "  (19999, 19653)\t0.016311524\n",
      "  (19999, 19743)\t0.010923061\n",
      "  (19999, 19875)\t0.017449116\n",
      "  (19999, 19923)\t0.06608872\n",
      "  (19999, 19930)\t0.059116583\n",
      "  (19999, 19932)\t0.035193272\n",
      "  (19999, 19937)\t0.02927337\n",
      "  (19999, 19940)\t0.028502084   (0, 106)\t0.015200946\n",
      "  (0, 205)\t0.03396554\n",
      "  (0, 380)\t0.024777729\n",
      "  (0, 536)\t0.030294023\n",
      "  (0, 538)\t0.04267204\n",
      "  (0, 582)\t0.027877681\n",
      "  (0, 678)\t0.027966406\n",
      "  (0, 752)\t0.04640913\n",
      "  (0, 772)\t0.092565075\n",
      "  (0, 970)\t0.031797167\n",
      "  (0, 979)\t0.029561343\n",
      "  (0, 1010)\t0.032000463\n",
      "  (0, 1180)\t0.06673276\n",
      "  (0, 1225)\t0.0293991\n",
      "  (0, 1227)\t0.016173096\n",
      "  (0, 1632)\t0.035048217\n",
      "  (0, 1684)\t0.03795532\n",
      "  (0, 1950)\t0.034112435\n",
      "  (0, 2124)\t0.012781092\n",
      "  (0, 2310)\t0.021116821\n",
      "  (0, 2416)\t0.02190713\n",
      "  (0, 2427)\t0.06673276\n",
      "  (0, 2718)\t0.09962525\n",
      "  (0, 2723)\t0.03701388\n",
      "  (0, 2736)\t0.049812626\n",
      "  :\t:\n",
      "  (4999, 17516)\t0.060507357\n",
      "  (4999, 17526)\t0.04993197\n",
      "  (4999, 17616)\t0.030077217\n",
      "  (4999, 17970)\t0.035022266\n",
      "  (4999, 18065)\t0.06035761\n",
      "  (4999, 18066)\t0.07989136\n",
      "  (4999, 18501)\t0.038919143\n",
      "  (4999, 18642)\t0.058610603\n",
      "  (4999, 18645)\t0.06946572\n",
      "  (4999, 19048)\t0.043691233\n",
      "  (4999, 19099)\t0.072890356\n",
      "  (4999, 19100)\t0.06262786\n",
      "  (4999, 19237)\t0.033982683\n",
      "  (4999, 19297)\t0.027145812\n",
      "  (4999, 19320)\t0.053729422\n",
      "  (4999, 19353)\t0.06792661\n",
      "  (4999, 19357)\t0.03783426\n",
      "  (4999, 19370)\t0.055429544\n",
      "  (4999, 19423)\t0.012888653\n",
      "  (4999, 19606)\t0.05037985\n",
      "  (4999, 19609)\t0.0654438\n",
      "  (4999, 19743)\t0.060315736\n",
      "  (4999, 19757)\t0.06573397\n",
      "  (4999, 19889)\t0.04950428\n",
      "  (4999, 19940)\t0.026230806\n"
     ]
    }
   ],
   "source": [
    "print(x_train, x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
